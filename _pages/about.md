---
layout: about
title: about
permalink: /
subtitle: <b>Principal Researcher</b> | <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research India</a>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>ðŸš€<b><i>"Making causal AI a reality"</i></b></p>
    <p>ðŸ’»<a href="https://pywhy.org">DoWhy/PyWhy OSS ecosystem</a></p>
    <hr />
    <p>ðŸ“–<a href="https://analyticsindiamag.com/ai-features/why-amit-sharma-created-dowhy/">Why Amit Sharma created DoWhy</a></p>
    <p>ðŸŽ§<a href="https://open.spotify.com/episode/6Rc3rZsAfcNGOQFXop7p0P">Causal Science|Humans of AI</a></p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
tabs: true
---

I'm a machine learning researcher working on improving the reasoning of AI systems. 

My work combines two seemingly incompatible ideas: the messy but generalizable capabilities of language models with the principled but rigid capabilities of causal models (or formal reasoning models). Early in 2023, I saw the potential of large language models (LLMs) for inferring causal relationships, a key part of scientific discovery. This has led to LLM-based algorithms that achieve up to [96% accuracy on inferring cause and effect](https://arxiv.org/abs/2305.00050) across scientific fields, including medicine (Covid-19), climate science (Arctic sea ice coverage), and engineering. I am now extending this work to build a causal AI assistant for science (see [PyWhy-LLM](https://github.com/py-why/pywhyllm)). 

On the other end, I work on how causality can help improve reliability of AI models. This has led to open-source tools such as [DoWhy](https://github.com/py-why/dowhy) for causal reasoning and [DiCE](https://github.com/interpretml/DiCE/) for counterfactual explanation that are widely used around the world. These days, I'm most excited by [Axiomatic Training](https://arxiv.org/abs/2407.07612), a framework for building reasoning verifiers that can correct a language model's output in real-time. Early results on causal reasoning tasks show that even a small model with 8 billion parameters can achieve nearly double the accuracy compared to frontier LLMs.

I'm also passionate about designing technology interventions that can have a positive societal impact (e.g., see [MindNotes app](https://mindnotes.nimhans.ac.in/)). 
If you are interested in working with me at MSR India, drop me an email. We hire interns throughout the year. There are also [postdoctoral positions](https://www.microsoft.com/en-us/research/msr-india-hiring/) available. Additionally,
if you are an undergraduate or a masters student, <!--you can additionally apply to the pre-doctoral [Research Fellowship](link-to-rf) program.-->
our lab runs an excellent pre-doctoral [Research Fellows](https://www.microsoft.com/en-us/research/academic-program/research-fellows-program-at-microsoft-research-india/) program.

{% tabs educ %}

{% tab educ Education %}

<b>[2015]</b> <b>Ph.D.</b> in Computer Science,
<a href="#">Cornell University</a>

<b>[2010]</b> <b>B.Tech.</b> in Computer Science,
<a href="#">IIT Kharagpur</a>

{% endtab %}

{% tab educ Interests %}

Causal inference \| Causality and machine learning

AI reasoning \| Accelerating scientific discovery

{% endtab %}

{% endtabs %}
